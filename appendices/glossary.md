# Glossary
# Glossary of Key Terms

## A
**Adversarial Debiasing**: Technical approach using a competing network (discriminator) to remove protected attribute information from learned representations

**Algorithmic Impact Assessment (AIA)**: A structured evaluation process required by the Canadian government to assess the potential impacts of automated decision systems

## C
**Counterfactual Fairness**: A prediction is fair if it would remain the same for an individual if their protected attribute were changed, holding all other non-descendant variables constant

**Cross-Modal Amplification**: Phenomenon where biases from different data modalities (e.g., text, vision) reinforce each other in multi-modal AI systems

## D
**Demographic Parity**: Fairness metric requiring equal positive prediction rates across demographic groups

**Disparate Impact**: A substantially different rate of selection in hiring, promotion, or other employment decisions that works to the disadvantage of members of a protected group

## E
**Equal Opportunity**: Fairness metric requiring equal true positive rates (sensitivity) across demographic groups

**Equalized Odds**: Fairness metric requiring both equal true positive rates and equal false positive rates across demographic groups

**EU AI Act**: The European Union's regulatory framework for artificial intelligence, establishing a risk-based approach to AI governance

## F
**Fairness Decision Record (FDR)**: Structured document capturing context, alternatives, rationale, stakeholders, and trade-offs for fairness decisions

**Feedback Loop Bias**: Dynamic amplification of initial biases through system-user interaction over time (especially in recommendation systems)

## G
**GDPR Article 22**: Provision in the General Data Protection Regulation giving individuals the right not to be subject to purely automated decision-making

## I
**Inherent Risk**: Potential harm associated with an AI system before any mitigating controls are applied

**Intersectionality**: Framework recognizing that individuals with multiple marginalized identities (e.g., race + gender) experience unique forms of discrimination not captured by single-dimension analysis

## P
**Proxy Variable**: Feature correlated with a protected attribute that indirectly encodes demographic information (e.g., zip code as proxy for race)

**Protected Attributes**: Characteristics that cannot be used as the basis for decisions under anti-discrimination laws (e.g., race, gender, age, disability status)

## R
**RACI Matrix**: Decision-making framework defining who is Responsible, Accountable, Consulted, and Informed for each decision type

**Representation Entanglement**: Phenomenon where deep learning models implicitly encode protected attributes in latent representations despite explicit removal from inputs

**Residual Risk**: Remaining risk after controls and mitigations have been applied

**RLHF (Reinforcement Learning from Human Feedback)**: Technique for fine-tuning AI models using human preferences as reward signals

## S
**SAFE Framework**: Framework for defining fairness requirements (Specific attributes, Actionable definition, Feature integration, Expected measures)

**Stereotype Threat**: Psychological phenomenon where individuals underperform due to anxiety about confirming negative stereotypes about their group

## T
**Technical Documentation**: Comprehensive documentation required by the EU AI Act covering an AI system's design, development, and performance

**Transparency Obligations**: Regulatory requirements to provide information about AI system capabilities, limitations, and decision-making processes

## Additional Terms

**Bias-Variance Fairness Trade-off**: The tension between reducing bias in model predictions and maintaining overall model performance

**Calibration**: Property where predicted probabilities match actual outcome rates across different groups

**Causal Fairness**: Approaches to fairness that consider the causal mechanisms generating data and outcomes

**Disparate Treatment**: Intentional discrimination against individuals based on protected characteristics

**Exploration-Exploitation Trade-off**: Balance in recommendation systems between showing familiar content and discovering new preferences

**Fair Representation Learning**: Techniques to learn feature representations that are invariant to protected attributes

**Group Fairness**: Approaches that ensure equitable outcomes across demographic groups

**Individual Fairness**: Principle that similar individuals should receive similar predictions

**Model Cards**: Standardized documentation for machine learning models that disclose performance characteristics and limitations

**Multi-Stakeholder Fairness**: Consideration of fairness across different parties affected by an AI system (users, providers, platform)

**Pre-training Data Bias**: Biases present in the large datasets used to train foundation models

**Red-teaming**: Systematic testing of AI systems using adversarial approaches to identify vulnerabilities

**Representation Learning**: Process where machine learning models automatically discover representations needed for feature detection or classification

**Tiered Alert Framework**: Structured approach to categorizing and responding to fairness issues based on severity

**Transfer Learning Inheritance**: Biases that are carried over from pre-trained models to downstream applications